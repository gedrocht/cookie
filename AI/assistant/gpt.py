import openai
import time
# from tts import speak
from random import random, randint, choice
import flux
import subprocess
import word_generator
import re
import threading
from groq import Groq
import os

from image_describer import _IMAGE_DESCRIBER
from summarizer import _SUMMARIZER
from automated_news import _AUTOMATED_NEWS
from action_taker import _ACTION_TAKER
from symbols_only import _SYMBOLS_ONLY
from fact_reporter import _FACT_REPORTER
from storyteller import _STORYTELLER
from observer import _OBSERVER
from config import _DEBUG_MODE, LOG_ROLLED_PROMPT_PARAMS, LOG_GENERATED_TEXT, _AI_OPTION_OPENAI, _AI_OPTION_GEMINI, _AI_OPTION_PAWANKRD, _AI_OPTION_GROQ, _AI_BEING_USED
from utils import roll, pick, join, select_portion_of_text
from piper_tts import speak

import google.generativeai as gemini
openai.api_key = "pk-iwRtLRpOGLEOdLZPXRsZdLdfCyWbHENkAytQYaRGWXFiohzW"
openai.base_url = "https://api.pawan.krd/cosmosrp-it/v1/chat/completions"
# Install the library first: pip install google-generativeai

'''
def epeak_tts(text):
    # Randomize speed (words per minute), pitch, and voice
    speed = randint(140, 170)  # Speech rate (default: 175)
    pitch = randint(35, 66)    # Pitch (default: 50)
#    voice = choice(['en+m1', 'en+m2'])  # Select random voice
    # Run espeak asynchronously (non-blocking)
    process = subprocess.Popen(['espeak', '-s', str(speed), '-p', str(pitch), text]) #, '-v', voice, text])
    return process
'''

# Create an image using only basic shapes and lines with similar to diagrams or pictograms. 
_img_prompt_beginning = '''
This image should be in night mode. Use iconography and clean design.
The composition of the image should be very clean, ordered, and understandable at a glance.
The image should be flat and 2D, using only basic geometric forms to convey the key elements visually. 
The imagery should have a bold, clear, and striking design to ensure immediate attention and recognition.
The style of the image should be characterized by bold colors, minimal detail, strong contrast, and aggressive, angular lines.
The overall vibe should urgent, direct, and somewhat intimidating, designed to prompt immediate caution and awareness.

The image must be composed of thin, glowing lines, often in green, white, or amber, arranged in geometric patterns or wireframe structures. These lines stand out sharply against a dark, usually black, background. The glow around the lines gives them a soft, ethereal appearance, creating a contrast between the crisp edges of the shapes and the blurred glow radiating from them.

The must be visuals are minimalist and skeletal, with objects formed by just their outlines, lacking any solid fills or textures. The lines themselves may be continuous or broken, and the overall aesthetic feels flat, with no shading, depth, or complex lighting effects. The geometry is rigid, often comprised of straight lines, grids, polygons, and angular forms, giving a very structured, mechanical feel. The glow can sometimes produce a flickering or afterimage effect, enhancing the sense of being generated by old, analog hardware.
'''

def use_google_instead(query, prompt, max_tokens):
    while True:
      try:
        result = gemini.GenerativeModel('gemini-1.5-flash').generate_content(f"{prompt}\n{query}")
        if result is None:
            print("[LLM] - Error: result is None")
            time.sleep(1)
            print("[LLM] - Retrying")
            continue
        if len(result.candidates) == 0:
            print("[LLM] - Error: len(result.candidates) == 0")
            time.sleep(1)
            print("[LLM] - Retrying")
            continue
        if len(result.candidates[0].content.parts) == 0:
            print("[LLM] - Error: len(result.candidates[0].content.parts) == 0")
            time.sleep(1)
            print("[LLM] - Retrying")
            continue
        return result.candidates[0].content.parts[0].text
      except Exception as e:
        print(f"[ERROR] - LLM ERROR: {e}")
        print(f"[DEBUG] - Retrying...")
        time.sleep(60)

groq_api_key = os.environ.get("GROQ_API_KEY")
groq_url = "https://api.groq.com/openai/v1/models"

groq_client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

def use_groq(query, prompt, max_tokens):
    chat_completion = groq_client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": query,
            },
            {
                "role": "system",
                "content": prompt
            }
        ],
        # model="llama3-8b-8192",
        model="llama3-70b-8192"
    )
    return chat_completion.choices[0].message.content

def get_chatgpt_response(query, prompt=_AUTOMATED_NEWS, max_tokens=200):
    if _DEBUG_MODE:
        return "Emergency. Alert. Warning. "
    time.sleep(0.33)
    return use_groq(query, prompt, max_tokens)
    # return use_google_instead(query, prompt, max_tokens)
    # time.sleep(0.1)
    # return "warning 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20"
    while True:
        start_time = time.time()
        try:
            response = openai.chat.completions.create(
    #            model=pick(["gpt-3.5-turbo-0125", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-16k"]) , # Use GPT-3.5-turbo for chat
                model="cosmosrp",
                messages=[
                    {"role": "system", "content":prompt},
                    {"role": "user", "content": query}],
    #            max_tokens=max_tokens,  # Adjust this as needed

    #            temperature=0.4 + random()*0.2,  # Controls the randomness of the response
                temperature=1.2
            )
            output = response.choices[0].message.content
            end_time = time.time()
            print(f"[LLM] - Request completed in {round(end_time-start_time)} seconds")
            return output
        except Exception as e:
            print(f"[ERROR] - LLM ERROR: {e}")
            print(f"[DEBUG] - Retrying...")
            time.sleep(1)

def load_wordfile(name):
    words = []
    with open(f"{name}.txt", "r") as file:
        # Read all lines from the file into a list
        strings = file.readlines()

    # Remove the newline characters from each string
    words = [line.strip() for line in strings]
    return words

if __name__ == "__main__":
    global tts_queue_empty
    global tts_process
    global tts_duration
    global tts_time_started
    tts_process = None
    tts_queue_empty = True
    tts_duration = 0
    tts_time_started = None
    print("[DEBUG] - Initializing Flux")
    flux_data = flux.init()
    print("[DEBUG] - Initializing word generator")
    word_generator.init()
    print("[DEBUG] - Loading words")
    _MOODS = load_wordfile("moods")
    _PLOT_MOODS = load_wordfile("plot_moods")
    _PLOTS = load_wordfile("plots")
    _STAKES = load_wordfile("stakes")
    _SUBPLOTS = load_wordfile("subplots")
    _SUBPLOT_STAKES_BEGINNINGS = load_wordfile("subplot_stakes_beginnings")
    _SUBPLOT_STAKES_ENDS = load_wordfile("subplot_stakes_ends")
    _STAKE_MULTIPLIERS = load_wordfile("stake_multipliers")
    # story_beginning = input("Enter LLM query: ")
    previous_storyteller_text = "The sky is on fire, and something in the air is making people act strangely."
    previous_summarizer_text = "It was previously reported that the atmosphere had ignited and the air was causing unusual behavior."
    action_text = "I will stay indoors, locking my doors and windows. I will avoid the people acting strangely."
    while True:
        print(f"[LLM] - Requesting story summary from _SUMMARIZER")
        summarizer_text = get_chatgpt_response(f"{previous_summarizer_text} {previous_storyteller_text}", _SUMMARIZER())
        # \n\nRANDOM INDIVIDUAL'S ACTIONS: {action_text}", 
        print(f"[LLM] - Requesting new story from _STORYTELLER")
        storyteller_text = get_chatgpt_response(f"PREVIOUS SUMMARY:\n{summarizer_text}.",
                                                _STORYTELLER(_MOODS, _PLOT_MOODS, _PLOTS, _STAKES, _SUBPLOTS, _SUBPLOT_STAKES_BEGINNINGS, _SUBPLOT_STAKES_ENDS, _STAKE_MULTIPLIERS), 400)
        if LOG_GENERATED_TEXT:
            log_output = ""
            log_output += f"[SUMMARIZER] - {summarizer_text}"
            print(log_output.replace("\n","").replace("\r",""))
            log_output = ""
            log_output += f"[STORYTELLER] - {storyteller_text}"
            print(log_output.replace("\n","").replace("\r",""))
        previous_summarizer_text = summarizer_text
        previous_storyteller_text = storyteller_text
        num_observations = 1
        if roll(0.15):
            num_observations = 2
        for obs_i in range(0,num_observations):
            print(f"[LLM] - Requesting observer text from _OBSERVER")

            obs_input_raw = f"{summarizer_text} {storyteller_text}"
            observer_input_text = obs_input_raw
            if num_observations > 1:
                observer_input_text = select_portion_of_text((obs_i+1)/(num_observations+1), obs_input_raw)

            observer_text = get_chatgpt_response(f"{observer_input_text}", _OBSERVER(obs_i, num_observations), 333)
            if LOG_GENERATED_TEXT:
                log_output = ""
                log_output += f"[OBSERVER] - {observer_text}"
                print(log_output.replace("\n","").replace("\r",""))

            num_science_reports = 1
            if roll(0.2):
                num_science_reports = 2
            if _AI_BEING_USED == _AI_OPTION_PAWANKRD:
                num_science_reports = 2
            for science_i in range(0, num_science_reports):
                print(f"[DEBUG] - Generating science report {science_i+1} of {num_science_reports}")
                print(f"[LLM] - Requesting science report from _FACT_REPORTER")
                scientist_input_text = observer_text 
                if num_science_reports > 1:
                    scientist_input_text = select_portion_of_text((science_i+1)/(num_science_reports+1), observer_text)
                scientist_text = get_chatgpt_response(scientist_input_text, _FACT_REPORTER(science_i, num_science_reports), 267)
                if LOG_GENERATED_TEXT:
                    log_output = ""
                    log_output += f"[SCIENTIST] - {scientist_text}"
                    print(log_output.replace("\n","").replace("\r",""))
                
                report = scientist_text

                if not tts_process is None:
                    while is_speaking(tts_process):
    #                     print("waiting for TTS")
                        time.sleep(1)
                while not tts_queue_empty:
    #                print("waiting for TTS")
                    time.sleep(1)

                num_announcements = 1
                if roll(0.25):
                    num_announcements = 2
                if _AI_BEING_USED == _AI_OPTION_PAWANKRD:
                    num_announcements = 3
                queued_announcements = []
                for announcement_i in range(0, num_announcements):
                    print(f"[DEBUG] - Generating announcement {announcement_i+1} of {num_announcements}")
                    announcement_input_text = report
                    if num_announcements > 1:
                        announcement_input_text = select_portion_of_text((announcement_i+1)/(num_announcements+1), report)
                    print(f"[LLM]- Requesting first pass of announcement from _AUTOMATED_NEWS")
                    announcement_prompt = _AUTOMATED_NEWS(announcement_i, num_announcements)
                    announcement_text = get_chatgpt_response(announcement_input_text, announcement_prompt)
                    print(f"[LLM] - Requesting second pass of announcement from _AUTOMATED_NEWS")
                    announcement_text = get_chatgpt_response(announcement_text, announcement_prompt)
                    announcement_text = re.sub(r'\*.*?\*', '', announcement_text)
                    announcement_text = announcement_text.replace("*", "")
                    queued_announcements.append(announcement_text)
                
#                tts_queue_empty = False
#                thread = threading.Thread(target=speak_tts_queue, args=([queued_announcements]))
#                thread.start()

                for i in range(0, num_announcements):
                    announcement_text = queued_announcements[i]
                    speak(announcement_text)

                    if LOG_GENERATED_TEXT:
                        log_output = ""
                        log_output += f"[ANNOUNCEMENT] - {announcement_text}"
                        print(log_output.replace("\n","").replace("\r",""))

                    # action_text = get_chatgpt_response(announcement_text, _ACTION_TAKER())
                    # print("----------------------------------")
                    # print(f"[ACTION]\n{action_text}")
                    
                    # image_text = get_chatgpt_response(f"{scientist_text} {announcement_text}", _SYMBOLS_ONLY())
                    print(f"[LLM] - Requesting image description text from _IMAGE_DESCRIBER")
                    image_description_text = get_chatgpt_response(announcement_text, _IMAGE_DESCRIBER(), 300)
                    
                    if LOG_GENERATED_TEXT:
                        log_output = ""
                        log_output += f"[DESCRIPTION] - {image_description_text}"
                        print(log_output.replace("\n","").replace("\r",""))
                    
                    print(f"[LLM] - Requesting image text from _SYMBOLS_ONLY")
                    image_text = get_chatgpt_response(f"{image_description_text}", _SYMBOLS_ONLY(), 400)

                    num_images = 2
                    for image_i in range(0,num_images):
                        print(f"[DEBUG] - Generating image {image_i+1} of {num_images}")
                        flux.generate_image(flux_data, f"{_img_prompt_beginning} {image_text}")
                        # flux.generate_image(flux_data, f"{storyteller_text}")
                        
                        if LOG_GENERATED_TEXT:
                            log_output = ""
                            log_output += f"[IMAGE] - {_img_prompt_beginning} {image_text}"
                            print(log_output.replace("\n","").replace("\r",""))
